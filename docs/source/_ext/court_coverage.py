"""Sphinx extension for court coverage documentation.

This extension generates:
1. Index page with state courts map
2. Federal courts map page
3. Per-jurisdiction detail pages

It reads from the court registry (court_registry.toml) generated by
scripts/build_court_registry.py.
"""

from __future__ import annotations

try:
    import tomllib  # ty: ignore[unresolved-import]
except ImportError:
    import tomli as tomllib  # type: ignore[import-not-found,no-redef]

from pathlib import Path
from typing import TYPE_CHECKING, Any

from docutils import nodes
from docutils.parsers.rst import directives
from sphinx.application import Sphinx
from sphinx.util import logging
from sphinx.util.docutils import SphinxDirective

_logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    pass

# Registry data cache
_registry: dict | None = None

# State/territory full names (matches scripts/svg_paths.py)
STATE_NAMES: dict[str, str] = {
    "AK": "Alaska",
    "AL": "Alabama",
    "AR": "Arkansas",
    "AS": "American Samoa",
    "AZ": "Arizona",
    "CA": "California",
    "CO": "Colorado",
    "CT": "Connecticut",
    "DC": "District of Columbia",
    "DE": "Delaware",
    "FL": "Florida",
    "GA": "Georgia",
    "GU": "Guam",
    "HI": "Hawaii",
    "IA": "Iowa",
    "ID": "Idaho",
    "IL": "Illinois",
    "IN": "Indiana",
    "KS": "Kansas",
    "KY": "Kentucky",
    "LA": "Louisiana",
    "MA": "Massachusetts",
    "MD": "Maryland",
    "ME": "Maine",
    "MI": "Michigan",
    "MN": "Minnesota",
    "MO": "Missouri",
    "MP": "Northern Mariana Islands",
    "MS": "Mississippi",
    "MT": "Montana",
    "NC": "North Carolina",
    "ND": "North Dakota",
    "NE": "Nebraska",
    "NH": "New Hampshire",
    "NJ": "New Jersey",
    "NM": "New Mexico",
    "NV": "Nevada",
    "NY": "New York",
    "OH": "Ohio",
    "OK": "Oklahoma",
    "OR": "Oregon",
    "PA": "Pennsylvania",
    "PR": "Puerto Rico",
    "RI": "Rhode Island",
    "SC": "South Carolina",
    "SD": "South Dakota",
    "TN": "Tennessee",
    "TX": "Texas",
    "UT": "Utah",
    "VA": "Virginia",
    "VI": "Virgin Islands",
    "VT": "Vermont",
    "WA": "Washington",
    "WI": "Wisconsin",
    "WV": "West Virginia",
    "WY": "Wyoming",
}


def get_registry(app: Sphinx) -> dict:
    """Load and cache the court registry."""
    global _registry
    if _registry is None:
        registry_path = Path(app.srcdir) / "_generated" / "court_registry.toml"
        if registry_path.exists():
            with open(registry_path, "rb") as f:
                _registry = tomllib.load(f)
        else:
            _logger.warning(f"Court registry not found at {registry_path}")
            _registry = {"stats": {}, "scrapers": {}, "jurisdictions": {}}
    return _registry


class CourtStatsDirective(SphinxDirective):
    """Directive to display court coverage statistics.

    Usage::

        .. court-stats::
    """

    has_content = False
    required_arguments = 0
    optional_arguments = 0

    def run(self) -> list[nodes.Node]:
        """Generate statistics table."""
        registry = get_registry(self.env.app)
        stats = registry.get("stats", {})

        # Create a simple table
        table = nodes.table()
        tgroup = nodes.tgroup(cols=2)
        table += tgroup

        # Column specifications
        tgroup += nodes.colspec(colwidth=50)
        tgroup += nodes.colspec(colwidth=20)

        # Header
        thead = nodes.thead()
        tgroup += thead
        header_row = nodes.row()
        thead += header_row
        header_row += nodes.entry("", nodes.paragraph(text="Metric"))
        header_row += nodes.entry("", nodes.paragraph(text="Count"))

        # Body
        tbody = nodes.tbody()
        tgroup += tbody

        metrics = [
            ("Active scrapers", stats.get("total_scrapers", 0)),
            ("Known courts", stats.get("total_courts_known", 0)),
            ("Covered courts", stats.get("total_courts_covered", 0)),
        ]

        for metric, value in metrics:
            row = nodes.row()
            tbody += row
            row += nodes.entry("", nodes.paragraph(text=metric))
            row += nodes.entry("", nodes.paragraph(text=str(value)))

        return [table]


class JurisdictionListDirective(SphinxDirective):
    """Directive to list all jurisdictions with coverage info.

    Usage::

        .. jurisdiction-list::
           :type: state  # or "federal"
    """

    has_content = False
    required_arguments = 0
    optional_arguments = 0
    option_spec = {
        "type": directives.unchanged,
    }

    def run(self) -> list[nodes.Node]:
        """Generate jurisdiction list."""
        registry = get_registry(self.env.app)
        jurisdictions = registry.get("jurisdictions", {})
        jur_type = self.options.get("type", "state")

        # Filter jurisdictions
        if jur_type == "federal":
            filtered = {k: v for k, v in jurisdictions.items() if k == "FED"}
        else:
            filtered = {k: v for k, v in jurisdictions.items() if k != "FED"}

        if not filtered:
            return [nodes.paragraph(text="No jurisdictions found.")]

        # Create bullet list
        bullet_list = nodes.bullet_list()

        for jur_code, jur_data in sorted(filtered.items()):
            coverage = jur_data.get("coverage_pct", 0)
            courts = jur_data.get("courts", [])

            # Get full state/territory name
            jur_name = STATE_NAMES.get(jur_code, jur_code)

            item = nodes.list_item()
            para = nodes.paragraph()

            # Link to jurisdiction page with full name
            # Use relative path since this directive is used from courts/index.rst
            ref = nodes.reference(
                "",
                f"{jur_name} ({jur_code})",
                internal=True,
                refuri=f"{jur_code.lower()}.html",
            )
            para += ref
            para += nodes.Text(
                f" - {len(courts)} courts, {coverage * 100:.1f}% coverage"
            )

            item += para
            bullet_list += item

        return [bullet_list]


class ScraperInfoDirective(SphinxDirective):
    """Directive to display scraper information.

    Usage::

        .. scraper-info:: Site
    """

    has_content = False
    required_arguments = 1
    optional_arguments = 0

    def run(self) -> list[nodes.Node]:
        """Generate scraper info block."""
        scraper_id = self.arguments[0]
        registry = get_registry(self.env.app)
        scrapers = registry.get("scrapers", {})

        if scraper_id not in scrapers:
            return [nodes.paragraph(text=f"Scraper '{scraper_id}' not found.")]

        scraper = scrapers[scraper_id]
        result: list[nodes.Node] = []

        # Title
        title = nodes.rubric(text=scraper_id)
        result.append(title)

        # Field list for metadata
        field_list = nodes.field_list()

        fields = [
            ("Module", scraper.get("module_path", "")),
            ("Status", scraper.get("status", "unknown").title()),
            ("Version", scraper.get("version", "")),
            ("Last Verified", scraper.get("last_verified", "")),
            ("Oldest Record", scraper.get("oldest_record", "")),
            ("Source", scraper.get("court_url", "")),
            (
                "Rate Limit",
                (
                    f"{scraper.get('msec_per_request', 'N/A')}ms"
                    if scraper.get("msec_per_request")
                    else "None"
                ),
            ),
            (
                "Auth Required",
                "Yes" if scraper.get("requires_auth") else "No",
            ),
        ]

        for name, value in fields:
            if value:
                field = nodes.field()
                field += nodes.field_name(text=name)
                field_body = nodes.field_body()

                if name == "Source" and value.startswith("http"):
                    para = nodes.paragraph()
                    para += nodes.reference("", value, refuri=value)
                    field_body += para
                elif name == "Module":
                    para = nodes.paragraph()
                    para += nodes.literal(text=value)
                    field_body += para
                else:
                    field_body += nodes.paragraph(text=str(value))

                field += field_body
                field_list += field

        result.append(field_list)

        # Data types
        data_types = scraper.get("data_types", [])
        if data_types:
            para = nodes.paragraph()
            para += nodes.strong(text="Data Types: ")
            para += nodes.Text(", ".join(data_types))
            result.append(para)

        # Return type and model hierarchy
        return_type = scraper.get("return_type", "")
        parent_model = scraper.get("parent_model", "")
        if return_type:
            para = nodes.paragraph()
            para += nodes.strong(text="Return Type: ")
            para += nodes.literal(text=return_type)
            if parent_model:
                para += nodes.Text(" (extends ")
                para += nodes.literal(text=parent_model)
                para += nodes.Text(")")
            result.append(para)

        # Searchability
        searchability = scraper.get("searchability", {})
        if searchability:
            para = nodes.paragraph()
            para += nodes.strong(text="Searchability: ")

            # Format: ModelName(field1[DateRange], field2[SetFilter])
            model_parts = []
            for model_name, filters in searchability.items():
                field_parts = []
                for filter_type, fields in filters.items():
                    for field_name in fields:
                        # Abbreviate filter types for compactness
                        abbrev = {
                            "DateRange": "date",
                            "SetFilter": "set",
                            "UniqueMatch": "exact",
                        }.get(filter_type, filter_type)
                        field_parts.append(f"{field_name}[{abbrev}]")
                if field_parts:
                    model_parts.append(
                        f"{model_name}({', '.join(field_parts)})"
                    )

            para += nodes.Text("; ".join(model_parts))
            result.append(para)

        # Court IDs
        court_ids = scraper.get("court_ids", [])
        if court_ids:
            para = nodes.paragraph()
            para += nodes.strong(text="Courts: ")
            para += nodes.Text(", ".join(court_ids[:10]))
            if len(court_ids) > 10:
                para += nodes.Text(f" ... and {len(court_ids) - 10} more")
            result.append(para)

        # Docstring
        docstring = scraper.get("docstring", "")
        if docstring:
            result.append(nodes.paragraph(text=docstring))

        return result


class CourtTableDirective(SphinxDirective):
    """Directive to display court coverage table for a jurisdiction.

    Usage::

        .. court-table:: TX
    """

    has_content = False
    required_arguments = 1
    optional_arguments = 0

    def run(self) -> list[nodes.Node]:
        """Generate court coverage table."""
        jur_code = self.arguments[0].upper()
        registry = get_registry(self.env.app)
        jurisdictions = registry.get("jurisdictions", {})
        scrapers = registry.get("scrapers", {})
        court_names = registry.get("court_names", {})

        jur_data = jurisdictions.get(jur_code)
        if not jur_data:
            return [
                nodes.paragraph(text=f"Jurisdiction '{jur_code}' not found.")
            ]

        # Build coverage map: court_id -> {scraper_id, data_types, module_path}
        court_coverage: dict[str, dict] = {}
        for scraper_id, scraper in scrapers.items():
            for court_id in scraper.get("court_ids", []):
                if court_id not in court_coverage:
                    court_coverage[court_id] = {
                        "scraper": scraper_id,
                        "data_types": set(),
                        "module_path": scraper.get("module_path", ""),
                    }
                court_coverage[court_id]["data_types"].update(
                    scraper.get("data_types", [])
                )

        courts = jur_data.get("courts", [])
        if not courts:
            return [nodes.paragraph(text="No courts in this jurisdiction.")]

        # Create table
        table = nodes.table()
        tgroup = nodes.tgroup(cols=6)
        table += tgroup

        # Column specs: Court Name, Court ID, Opinions, Dockets, Oral Args, Scraper
        for width in [50, 20, 10, 10, 10, 15]:
            tgroup += nodes.colspec(colwidth=width)

        # Header
        thead = nodes.thead()
        tgroup += thead
        header_row = nodes.row()
        thead += header_row

        for header in [
            "Court Name",
            "Court ID",
            "Opinions",
            "Dockets",
            "Oral Args",
            "Scraper",
        ]:
            header_row += nodes.entry("", nodes.paragraph(text=header))

        # Body
        tbody = nodes.tbody()
        tgroup += tbody

        for court_id in sorted(courts):
            row = nodes.row()
            tbody += row

            # Court Name (from registry, fallback to court_id)
            court_name = court_names.get(court_id, court_id)
            row += nodes.entry("", nodes.paragraph(text=court_name))

            # Court ID
            row += nodes.entry("", nodes.paragraph(text=court_id))

            # Coverage columns
            coverage = court_coverage.get(court_id, {})
            data_types = coverage.get("data_types", set())

            for dtype in ["opinions", "dockets", "oral_arguments"]:
                cell = nodes.entry()
                if dtype in data_types:
                    cell += nodes.paragraph(text="\u2714")  # Check mark
                else:
                    cell += nodes.paragraph(text="\u2718")  # Cross mark
                row += cell

            # Scraper (with link to autodoc if module_path exists)
            scraper_id = coverage.get("scraper", "-")
            module_path = coverage.get("module_path", "")
            cell = nodes.entry()
            para = nodes.paragraph()
            if module_path:
                # Create link to autodoc-generated module page
                # URL format: api/{module_path}.html
                ref = nodes.reference(
                    "",
                    scraper_id,
                    internal=True,
                    refuri=f"../api/{module_path}.html",
                )
                para += ref
            else:
                para += nodes.Text(scraper_id)
            cell += para
            row += cell

        return [table]


def generate_court_pages(app: Sphinx) -> None:
    """Generate court coverage RST pages.

    This runs during the builder-inited event to create
    jurisdiction-specific pages.
    """
    registry = get_registry(app)
    jurisdictions = registry.get("jurisdictions", {})

    # Create courts directory
    courts_dir = Path(app.srcdir) / "courts"
    courts_dir.mkdir(exist_ok=True)

    # Generate index page for courts
    # Build list of jurisdiction codes for toctree
    jur_codes = sorted([k.lower() for k in jurisdictions if k != "FED"])
    jur_toctree = "\n   ".join(jur_codes)

    index_content = f""".. _court-coverage:

Court Coverage
==============

`View Federal Courts Map <federal.html>`_

The map below shows Juriscraper's coverage of state and territorial courts.
Click any state or territory to see detailed scraper information.

.. raw:: html
   :file: ../_static/state_coverage.svg

Legend
------

Coverage is shown as a grayscale gradient from white (0%) to black (100%).
States with diagonal stripes have no court information in our database yet.

Coverage Statistics
-------------------

.. court-stats::

State Jurisdictions
-------------------

.. jurisdiction-list::
   :type: state

.. toctree::
   :maxdepth: 1
   :hidden:

   federal
   {jur_toctree}
"""
    (courts_dir / "index.rst").write_text(index_content)

    # Federal circuit definitions
    federal_circuits = {
        "1": {
            "name": "First Circuit",
            "states": ["ME", "NH", "MA", "RI", "PR"],
        },
        "2": {"name": "Second Circuit", "states": ["VT", "NY", "CT"]},
        "3": {"name": "Third Circuit", "states": ["NJ", "PA", "DE", "VI"]},
        "4": {
            "name": "Fourth Circuit",
            "states": ["MD", "WV", "VA", "NC", "SC"],
        },
        "5": {"name": "Fifth Circuit", "states": ["TX", "LA", "MS"]},
        "6": {"name": "Sixth Circuit", "states": ["OH", "MI", "KY", "TN"]},
        "7": {"name": "Seventh Circuit", "states": ["WI", "IL", "IN"]},
        "8": {
            "name": "Eighth Circuit",
            "states": ["ND", "SD", "NE", "MN", "IA", "MO", "AR"],
        },
        "9": {
            "name": "Ninth Circuit",
            "states": [
                "WA",
                "OR",
                "CA",
                "NV",
                "ID",
                "MT",
                "AZ",
                "AK",
                "HI",
                "GU",
                "MP",
            ],
        },
        "10": {
            "name": "Tenth Circuit",
            "states": ["WY", "UT", "CO", "KS", "OK", "NM"],
        },
        "11": {"name": "Eleventh Circuit", "states": ["AL", "GA", "FL"]},
        "DC": {"name": "D.C. Circuit", "states": ["DC"]},
    }

    # Build circuit toctree
    circuit_codes = ["ca" + c.lower() for c in federal_circuits]
    circuit_toctree = "\n   ".join(circuit_codes)

    # Generate federal index page
    federal_content = """.. _federal-courts:

Federal Court Coverage
======================

`View State Courts Map <index.html>`_

The map below shows Juriscraper's coverage of federal courts by circuit.

.. raw:: html
   :file: ../_static/federal_coverage.svg

Circuits
--------

"""
    for circuit_num, circuit_info in federal_circuits.items():
        states_list = ", ".join(circuit_info["states"])
        federal_content += f"- `{circuit_info['name']} <ca{circuit_num.lower()}.html>`_ ({states_list})\n"

    federal_content += f"""

.. toctree::
   :maxdepth: 1
   :hidden:

   {circuit_toctree}
"""
    (courts_dir / "federal.rst").write_text(federal_content)

    # Generate per-circuit federal pages
    for circuit_num, circuit_info in federal_circuits.items():
        circuit_name = circuit_info["name"]
        states_list = ", ".join(circuit_info["states"])

        page_content = f""".. _federal-ca{circuit_num.lower()}:

{circuit_name}
{"=" * len(circuit_name)}

`Federal Map <federal.html>`_ | `State Map <index.html>`_

**States/Territories:** {states_list}

Federal Courts in this Circuit
------------------------------

*Federal court scraper information will be displayed here when available.*

"""
        (courts_dir / f"ca{circuit_num.lower()}.rst").write_text(page_content)

    # Generate per-jurisdiction pages
    for jur_code, jur_data in jurisdictions.items():
        if jur_code == "FED":
            continue  # Skip federal, handled separately

        # Get full state/territory name
        jur_name = STATE_NAMES.get(jur_code, jur_code)
        title = f"{jur_name} Courts"

        page_content = f""".. _courts-{jur_code.lower()}:

{title}
{"=" * len(title)}

`State Map <index.html>`_ | `Federal Map <federal.html>`_

Court Coverage
--------------

.. court-table:: {jur_code}

Scrapers
--------

"""
        # Find scrapers for this jurisdiction
        scrapers = registry.get("scrapers", {})
        jur_courts = set(jur_data.get("courts", []))

        added_scrapers = set()
        for scraper_id, scraper in scrapers.items():
            scraper_courts = set(scraper.get("court_ids", []))
            if (
                scraper_courts & jur_courts
                and scraper_id not in added_scrapers
            ):
                page_content += f".. scraper-info:: {scraper_id}\n\n"
                added_scrapers.add(scraper_id)

        if not added_scrapers:
            page_content += "*No scrapers available for this jurisdiction.*\n"

        (courts_dir / f"{jur_code.lower()}.rst").write_text(page_content)


def setup(app: Sphinx) -> dict[str, Any]:
    """Set up the Sphinx extension."""
    # Register directives
    app.add_directive("court-stats", CourtStatsDirective)
    app.add_directive("jurisdiction-list", JurisdictionListDirective)
    app.add_directive("scraper-info", ScraperInfoDirective)
    app.add_directive("court-table", CourtTableDirective)

    # Register event handler to generate pages
    app.connect("builder-inited", generate_court_pages)

    return {
        "version": "0.1",
        "parallel_read_safe": True,
        "parallel_write_safe": True,
    }
